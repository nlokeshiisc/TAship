# -*- coding: utf-8 -*-
"""AddBinaryStrings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1np0q8E2e7yJGXuKCYfkQQa8mPSBqKWvP

$\alpha$
"""

import torch
import torch.nn as nn
import torch.optim as optim
torch.manual_seed(42)

DEBUG = True

def generate_binary_batch(batch_size, num_bits=10):
    # Generate random 10-bit binary numbers
    binary_numbers = torch.randint(2, size=(batch_size, num_bits), dtype=torch.int)

    return binary_numbers

if DEBUG == True:
    batch_size = 5
    binary_batch = generate_binary_batch(batch_size)
    print("Generated Binary Batch:")
    print(binary_batch)

input_1 = generate_binary_batch(batch_size=10)
input_2 = generate_binary_batch(batch_size=10)

def binary_to_decimal(binary_tensor):
    # Convert binary tensor to decimal
    decimal_tensor = torch.zeros(binary_tensor.size(0), dtype=torch.int)
    for i in range(binary_tensor.size(1)):
        decimal_tensor = decimal_tensor * 2 + binary_tensor[:, i]

    return decimal_tensor

def decimal_to_binary(decimal_tensor, num_bits=10):
    # Convert decimal tensor to binary with zero-padding
    binary_tensor = torch.zeros(decimal_tensor.size(0), num_bits, dtype=torch.int)
    for i in range(num_bits - 1, -1, -1):
        binary_tensor[:, i] = decimal_tensor % 2
        decimal_tensor = decimal_tensor // 2

    return binary_tensor

def decimal_addition(input_1, input_2):
    # Convert binary inputs to decimal
    decimal_input_1 = binary_to_decimal(input_1)
    decimal_input_2 = binary_to_decimal(input_2)

    # Perform decimal addition
    decimal_result = decimal_input_1 + decimal_input_2

    # Convert decimal result back to padded binary
    binary_result = decimal_to_binary(decimal_result)

    return binary_result

target_batch = decimal_addition(input_1=input_1, input_2=input_2)

if DEBUG == True:
    for i in range(3):
        print("i0:", input_1[i])
        print("i1:", input_2[i])
        print("o :", target_batch[i])

import torch

def concatenate_inputs(input_1, input_2):
    # Concatenate input_2 and input_1 along the last dimension
    input_batch = torch.cat([input_1.unsqueeze(2), input_2.unsqueeze(2)], dim=2)
    input_batch = torch.flip(input_batch, dims=[1])
    return input_batch

input_batch = concatenate_inputs(input_1, input_2)
target_batch =  torch.flip(target_batch, dims=[1]).unsqueeze(-1)

if DEBUG == True:
    for i in range(3):
        print("i:", input_batch[i])
        print("o:", target_batch[i])
    print("input_batch", input_batch.shape)
    print("target_batch", target_batch.shape)

def generate_batch(batch_size=16, num_bits=10):
    input_1 = generate_binary_batch(batch_size=batch_size, num_bits=num_bits)
    input_2 = generate_binary_batch(batch_size=batch_size, num_bits=num_bits)

    target_batch = decimal_addition(input_1=input_1, input_2=input_2)

    input_batch = concatenate_inputs(input_1, input_2).to(torch.float32)
    target_batch =  torch.flip(target_batch, dims=[1]).unsqueeze(-1).to(torch.float32)
    return input_batch, target_batch

class FullAdderRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FullAdderRNN, self).__init__()

        # Define an RNN layer
        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)

        # Define a fully connected layer for output
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, input_sequence):
        # Input sequence shape: (batch_size, sequence_length, input_size)
        # Output shape: (batch_size, sequence_length, output_size)

        # RNN forward pass
        rnn_output, _ = self.rnn(input_sequence)

        # Fully connected layer for output
        output = self.fc(rnn_output)

        return output

# Hyperparameters
batch_size = 16
sequence_length = 10
input_size = 2
hidden_size = 4
output_size = 1
learning_rate = 0.001
num_epochs = 10000

# Instantiate the model, loss function, and optimizer
model = FullAdderRNN(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    # Generate random data for each epoch
    input_data, target_data = generate_batch(batch_size)

    # Forward pass
    output_sequence = model(input_data)

    # Compute the loss
    loss = criterion(output_sequence, target_data)

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print the loss every 100 epochs
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

test_data, test_target = generate_batch(batch_size=100, num_bits=20)
with torch.no_grad():
    test_preds = model(test_data)

# Function to convert real-valued outputs to binary
def convert_to_binary(outputs, threshold=0.5):
    binary_outputs = (outputs >= threshold).float()
    return binary_outputs

test_preds_binary = convert_to_binary(test_preds)
print(torch.nn.MSELoss()(test_preds_binary, test_target))

test_target.shape

Inference with more bits
Step activation on the output layer

Theory Questions:
  - make h as 1 bit and check if we can interpret the weights

parameterize the code

Bit serial comparison is x1 > x2?

Is x1 + x2 > 2^n

Expt with a large lr

model.rnn.state_dict()

